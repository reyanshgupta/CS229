{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42433b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Compute softmax function for a batch of input values. \n",
    "    The first dimension of the input corresponds to the batch size. The second dimension\n",
    "    corresponds to every class in the output. When implementing softmax, you should be careful\n",
    "    to only sum over the second dimension.\n",
    "\n",
    "    Important Note: You must be careful to avoid overflow for this function. Functions\n",
    "    like softmax have a tendency to overflow when very large numbers like e^10000 are computed.\n",
    "    You will know that your function is overflow resistent when it can handle input like:\n",
    "    np.array([[10000, 10010, 10]]) without issues.\n",
    "\n",
    "    Args:\n",
    "        x: A 2d numpy float array of shape batch_size x number_of_classes\n",
    "\n",
    "    Returns:\n",
    "        A 2d numpy float array containing the softmax results of shape batch_size x number_of_classes\n",
    "    \"\"\"\n",
    "    # *** START CODE HERE ***\n",
    "    max_x = np.max(x, axis=1, keepdims=True)\n",
    "    exp_x = np.exp(x - max_x)\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "    # *** END CODE HERE ***\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid function for the input here.\n",
    "\n",
    "    Args:\n",
    "        x: A numpy float array\n",
    "\n",
    "    Returns:\n",
    "        A numpy float array containing the sigmoid results\n",
    "    \"\"\"\n",
    "    # *** START CODE HERE ***\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "    # *** END CODE HERE ***\n",
    "\n",
    "def get_initial_params(input_size, num_hidden, num_output):\n",
    "    \"\"\"\n",
    "    Compute the initial parameters for the neural network.\n",
    "\n",
    "    This function should return a dictionary mapping parameter names to numpy arrays containing\n",
    "    the initial values for those parameters.\n",
    "\n",
    "    There should be four parameters for this model:\n",
    "    W1 is the weight matrix for the hidden layer of size input_size x num_hidden\n",
    "    b1 is the bias vector for the hidden layer of size num_hidden\n",
    "    W2 is the weight matrix for the output layers of size num_hidden x num_output\n",
    "    b2 is the bias vector for the output layer of size num_output\n",
    "\n",
    "    As specified in the PDF, weight matrices should be initialized with a random normal distribution\n",
    "    centered on zero and with scale 1.\n",
    "    Bias vectors should be initialized with zero.\n",
    "    \n",
    "    Args:\n",
    "        input_size: The size of the input data\n",
    "        num_hidden: The number of hidden states\n",
    "        num_output: The number of output classes\n",
    "    \n",
    "    Returns:\n",
    "        A dict mapping parameter names to numpy arrays\n",
    "    \"\"\"\n",
    "\n",
    "    # *** START CODE HERE ***\n",
    "    W1 = np.random.normal(0, 1, size=(num_hidden, input_size))\n",
    "    b1 = np.zeros(num_hidden)\n",
    "    W2 = np.random.normal(0, 1, size=(num_output, num_hidden))\n",
    "    b2 = np.zeros(num_output)\n",
    "    params = {'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\n",
    "    return params\n",
    "    # *** END CODE HERE ***\n",
    "\n",
    "def forward_prop(data, labels, params):\n",
    "    \"\"\"\n",
    "    Implement the forward layer given the data, labels, and params.\n",
    "    \n",
    "    Args:\n",
    "        data: A numpy array containing the input\n",
    "        labels: A 2d numpy array containing the labels\n",
    "        params: A dictionary mapping parameter names to numpy arrays with the parameters.\n",
    "            This numpy array will contain W1, b1, W2 and b2\n",
    "            W1 and b1 represent the weights and bias for the hidden layer of the network\n",
    "            W2 and b2 represent the weights and bias for the output layer of the network\n",
    "\n",
    "    Returns:\n",
    "        A 3 element tuple containing:\n",
    "            1. A numpy array of the activations (after the sigmoid) of the hidden layer\n",
    "            2. A numpy array The output (after the softmax) of the output layer\n",
    "            3. The average loss for these data elements\n",
    "    \"\"\"\n",
    "    # *** START CODE HERE ***\n",
    "    W1 = params['W1']\n",
    "    b1 = params['b1']\n",
    "    W2 = params['W2']\n",
    "    b2 = params['b2']\n",
    "\n",
    "    Z1 = np.dot(data, W1.T) + b1\n",
    "    A1 = sigmoid(Z1)\n",
    "    Z2 = np.dot(A1, W2.T) + b2\n",
    "    predictions = softmax(Z2)\n",
    "    loss = - np.sum(labels * np.log(predictions))\n",
    "    loss = loss / len(data)\n",
    "    return A1, predictions, loss\n",
    "\n",
    "    # *** END CODE HERE ***\n",
    "\n",
    "def backward_prop(data, labels, params, forward_prop_func):\n",
    "    \"\"\"\n",
    "    Implement the backward propegation gradient computation step for a neural network\n",
    "    \n",
    "    Args:\n",
    "        data: A numpy array containing the input\n",
    "        labels: A 2d numpy array containing the labels\n",
    "        params: A dictionary mapping parameter names to numpy arrays with the parameters.\n",
    "            This numpy array will contain W1, b1, W2 and b2\n",
    "            W1 and b1 represent the weights and bias for the hidden layer of the network\n",
    "            W2 and b2 represent the weights and bias for the output layer of the network\n",
    "        forward_prop_func: A function that follows the forward_prop API above\n",
    "\n",
    "    Returns:\n",
    "        A dictionary of strings to numpy arrays where each key represents the name of a weight\n",
    "        and the values represent the gradient of the loss with respect to that weight.\n",
    "        \n",
    "        In particular, it should have 4 elements:\n",
    "            W1, W2, b1, and b2\n",
    "    \"\"\"\n",
    "    # *** START CODE HERE ***\n",
    "    W2 = params['W2']\n",
    "    batch_len = len(data)\n",
    "\n",
    "    A1, predictions, loss = forward_prop_func(data, labels, params)\n",
    "    \n",
    "    dZ2 = predictions - labels\n",
    "    dW2 = np.dot(dZ2.T, A1) / batch_len\n",
    "    db2 = np.sum(dZ2, axis=0) / batch_len\n",
    "    \n",
    "    dA1 = np.dot(dZ2, params['W2'].T)\n",
    "    dZ1 = A1 * (1 - A1) * dA1\n",
    "    dW1 = np.dot(dZ1.T, data) / batch_len\n",
    "    db1 = np.sum(dZ1, axis=0) / batch_len\n",
    "    \n",
    "    grads = {'W1': dW1, 'b1': db1, 'W2': dW2, 'b2': db2}\n",
    "    return grads\n",
    "    # *** END CODE HERE ***\n",
    "\n",
    "\n",
    "def backward_prop_regularized(data, labels, params, forward_prop_func, reg):\n",
    "    \"\"\"\n",
    "    Implement the backward propegation gradient computation step for a neural network\n",
    "    \n",
    "    Args:\n",
    "        data: A numpy array containing the input\n",
    "        labels: A 2d numpy array containing the labels\n",
    "        params: A dictionary mapping parameter names to numpy arrays with the parameters.\n",
    "            This numpy array will contain W1, b1, W2 and b2\n",
    "            W1 and b1 represent the weights and bias for the hidden layer of the network\n",
    "            W2 and b2 represent the weights and bias for the output layer of the network\n",
    "        forward_prop_func: A function that follows the forward_prop API above\n",
    "        reg: The regularization strength (lambda)\n",
    "\n",
    "    Returns:\n",
    "        A dictionary of strings to numpy arrays where each key represents the name of a weight\n",
    "        and the values represent the gradient of the loss with respect to that weight.\n",
    "        \n",
    "        In particular, it should have 4 elements:\n",
    "            W1, W2, b1, and b2\n",
    "    \"\"\"\n",
    "    # *** START CODE HERE ***\n",
    "    gradients = backward_prop(data, labels, params, forward_prop_func)\n",
    "\n",
    "    gradients['W1'] += 2 * reg * params['W1']\n",
    "    gradients['W2'] += 2 * reg * params['W2']\n",
    "    \n",
    "    return gradients\n",
    "    # *** END CODE HERE ***\n",
    "\n",
    "def gradient_descent_epoch(train_data, train_labels, learning_rate, batch_size, params, forward_prop_func, backward_prop_func):\n",
    "    \"\"\"\n",
    "    Perform one epoch of gradient descent on the given training data using the provided learning rate.\n",
    "\n",
    "    This code should update the parameters stored in params.\n",
    "    It should not return anything\n",
    "\n",
    "    Args:\n",
    "        train_data: A numpy array containing the training data\n",
    "        train_labels: A numpy array containing the training labels\n",
    "        learning_rate: The learning rate\n",
    "        batch_size: The amount of items to process in each batch\n",
    "        params: A dict of parameter names to parameter values that should be updated.\n",
    "        forward_prop_func: A function that follows the forward_prop API\n",
    "        backward_prop_func: A function that follows the backwards_prop API\n",
    "\n",
    "    Returns: This function returns nothing.\n",
    "    \"\"\"\n",
    "\n",
    "    # *** START CODE HERE ***\n",
    "    epoch_batches = int(np.ceil(len(train_data) / batch_size))\n",
    "    # Batchify training data using numpy.array_split\n",
    "    batches = np.array_split(train_data, num_batches)\n",
    "    label_batches = np.array_split(train_labels, num_batches)\n",
    "\n",
    "    # The training loop over batches\n",
    "    for batch_data, batch_labels in zip(batches, label_batches):\n",
    "        grads = backward_prop_func(batch_data, batch_labels, params, forward_prop_func)\n",
    "        # Update params using numpy.add\n",
    "        for wt in ['W1', 'b1', 'W2', 'b2']:\n",
    "            params[wt] -= learning_rate * grads[wt]\n",
    "    # *** END CODE HERE ***\n",
    "\n",
    "    # This function does not return anything\n",
    "    return\n",
    "\n",
    "def nn_train(\n",
    "    train_data, train_labels, dev_data, dev_labels, \n",
    "    get_initial_params_func, forward_prop_func, backward_prop_func,\n",
    "    num_hidden=300, learning_rate=5, num_epochs=30, batch_size=1000):\n",
    "\n",
    "    (nexp, dim) = train_data.shape\n",
    "\n",
    "    params = get_initial_params_func(dim, num_hidden, 10)\n",
    "\n",
    "    cost_train = []\n",
    "    cost_dev = []\n",
    "    accuracy_train = []\n",
    "    accuracy_dev = []\n",
    "    for epoch in range(num_epochs):\n",
    "        gradient_descent_epoch(train_data, train_labels, \n",
    "            learning_rate, batch_size, params, forward_prop_func, backward_prop_func)\n",
    "\n",
    "        h, output, cost = forward_prop_func(train_data, train_labels, params)\n",
    "        cost_train.append(cost)\n",
    "        accuracy_train.append(compute_accuracy(output,train_labels))\n",
    "        h, output, cost = forward_prop_func(dev_data, dev_labels, params)\n",
    "        cost_dev.append(cost)\n",
    "        accuracy_dev.append(compute_accuracy(output, dev_labels))\n",
    "\n",
    "    return params, cost_train, cost_dev, accuracy_train, accuracy_dev\n",
    "\n",
    "def nn_test(data, labels, params):\n",
    "    h, output, cost = forward_prop(data, labels, params)\n",
    "    accuracy = compute_accuracy(output, labels)\n",
    "    return accuracy\n",
    "\n",
    "def compute_accuracy(output, labels):\n",
    "    accuracy = (np.argmax(output,axis=1) == \n",
    "        np.argmax(labels,axis=1)).sum() * 1. / labels.shape[0]\n",
    "    return accuracy\n",
    "\n",
    "def one_hot_labels(labels):\n",
    "    one_hot_labels = np.zeros((labels.size, 10))\n",
    "    one_hot_labels[np.arange(labels.size),labels.astype(int)] = 1\n",
    "    return one_hot_labels\n",
    "\n",
    "def read_data(images_file, labels_file):\n",
    "    x = np.loadtxt(images_file, delimiter=',')\n",
    "    y = np.loadtxt(labels_file, delimiter=',')\n",
    "    return x, y\n",
    "\n",
    "def run_train_test(name, all_data, all_labels, backward_prop_func, num_epochs, plot=True):\n",
    "    params, cost_train, cost_dev, accuracy_train, accuracy_dev = nn_train(\n",
    "        all_data['train'], all_labels['train'], \n",
    "        all_data['dev'], all_labels['dev'],\n",
    "        get_initial_params, forward_prop, backward_prop_func,\n",
    "        num_hidden=300, learning_rate=5, num_epochs=num_epochs, batch_size=1000\n",
    "    )\n",
    "\n",
    "    t = np.arange(num_epochs)\n",
    "\n",
    "    if plot:\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1)\n",
    "\n",
    "        ax1.plot(t, cost_train,'r', label='train')\n",
    "        ax1.plot(t, cost_dev, 'b', label='dev')\n",
    "        ax1.set_xlabel('epochs')\n",
    "        ax1.set_ylabel('loss')\n",
    "        if name == 'baseline':\n",
    "            ax1.set_title('Without Regularization')\n",
    "        else:\n",
    "            ax1.set_title('With Regularization')\n",
    "        ax1.legend()\n",
    "\n",
    "        ax2.plot(t, accuracy_train,'r', label='train')\n",
    "        ax2.plot(t, accuracy_dev, 'b', label='dev')\n",
    "        ax2.set_xlabel('epochs')\n",
    "        ax2.set_ylabel('accuracy')\n",
    "        ax2.legend()\n",
    "\n",
    "        fig.savefig('./' + name + '.pdf')\n",
    "\n",
    "    accuracy = nn_test(all_data['test'], all_labels['test'], params)\n",
    "    print('For model %s, got accuracy: %f' % (name, accuracy))\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "def main(plot=True):\n",
    "    parser = argparse.ArgumentParser(description='Train a nn model.')\n",
    "    parser.add_argument('--num_epochs', type=int, default=30)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    np.random.seed(100)\n",
    "    train_data, train_labels = read_data('./images_train.csv', './labels_train.csv')\n",
    "    train_labels = one_hot_labels(train_labels)\n",
    "    p = np.random.permutation(60000)\n",
    "    train_data = train_data[p,:]\n",
    "    train_labels = train_labels[p,:]\n",
    "\n",
    "    dev_data = train_data[0:10000,:]\n",
    "    dev_labels = train_labels[0:10000,:]\n",
    "    train_data = train_data[10000:,:]\n",
    "    train_labels = train_labels[10000:,:]\n",
    "\n",
    "    mean = np.mean(train_data)\n",
    "    std = np.std(train_data)\n",
    "    train_data = (train_data - mean) / std\n",
    "    dev_data = (dev_data - mean) / std\n",
    "\n",
    "    test_data, test_labels = read_data('./images_test.csv', './labels_test.csv')\n",
    "    test_labels = one_hot_labels(test_labels)\n",
    "    test_data = (test_data - mean) / std\n",
    "\n",
    "    all_data = {\n",
    "        'train': train_data,\n",
    "        'dev': dev_data,\n",
    "        'test': test_data\n",
    "    }\n",
    "\n",
    "    all_labels = {\n",
    "        'train': train_labels,\n",
    "        'dev': dev_labels,\n",
    "        'test': test_labels,\n",
    "    }\n",
    "    \n",
    "    baseline_acc = run_train_test('baseline', all_data, all_labels, backward_prop, args.num_epochs, plot)\n",
    "    reg_acc = run_train_test('regularized', all_data, all_labels, \n",
    "        lambda a, b, c, d: backward_prop_regularized(a, b, c, d, reg=0.0001),\n",
    "        args.num_epochs, plot)\n",
    "        \n",
    "    return baseline_acc, reg_acc\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
