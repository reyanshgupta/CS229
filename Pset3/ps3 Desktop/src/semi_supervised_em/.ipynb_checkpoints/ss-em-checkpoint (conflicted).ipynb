{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcc16a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "PLOT_COLORS = ['red', 'green', 'blue', 'orange']  # Colors for your plots\n",
    "K = 4           # Number of Gaussians in the mixture model\n",
    "NUM_TRIALS = 3  # Number of trials to run (can be adjusted for debugging)\n",
    "UNLABELED = -1  # Cluster label for unlabeled data points (do not change)\n",
    "\n",
    "\n",
    "def main(is_semi_supervised, trial_num):\n",
    "    \"\"\"Problem 3: EM for Gaussian Mixture Models (unsupervised and semi-supervised)\"\"\"\n",
    "    print('Running {} EM algorithm...'\n",
    "          .format('semi-supervised' if is_semi_supervised else 'unsupervised'))\n",
    "\n",
    "    # Load dataset\n",
    "    train_path = os.path.join('.', 'train.csv')\n",
    "    x_all, z_all = load_gmm_dataset(train_path)\n",
    "\n",
    "    # Split into labeled and unlabeled examples\n",
    "    labeled_idxs = (z_all != UNLABELED).squeeze()\n",
    "    x_tilde = x_all[labeled_idxs, :]   # Labeled examples\n",
    "    z_tilde = z_all[labeled_idxs, :]   # Corresponding labels\n",
    "    x = x_all[~labeled_idxs, :]        # Unlabeled examples\n",
    "\n",
    "    # *** START CODE HERE ***\n",
    "    # (1) Initialize mu and sigma by splitting the n_examples data points uniformly at random\n",
    "    # into K groups, then calculating the sample mean and covariance for each group\n",
    "    # (2) Initialize phi to place equal probability on each Gaussian\n",
    "    # phi should be a numpy array of shape (K,)\n",
    "    # (3) Initialize the w values to place equal probability on each Gaussian\n",
    "    # w should be a numpy array of shape (m, K)\n",
    "    # *** END CODE HERE ***\n",
    "\n",
    "    if is_semi_supervised:\n",
    "        w = run_semi_supervised_em(x, x_tilde, z_tilde, w, phi, mu, sigma)\n",
    "    else:\n",
    "        w = run_em(x, w, phi, mu, sigma)\n",
    "\n",
    "    # Plot your predictions\n",
    "    z_pred = np.zeros(n)\n",
    "    if w is not None:  # Just a placeholder for the starter code\n",
    "        for i in range(n):\n",
    "            z_pred[i] = np.argmax(w[i])\n",
    "\n",
    "    plot_gmm_preds(x, z_pred, is_semi_supervised, plot_id=trial_num)\n",
    "\n",
    "\n",
    "def run_em(x, w, phi, mu, sigma):\n",
    "    \"\"\"Problem 3(d): EM Algorithm (unsupervised).\n",
    "\n",
    "    See inline comments for instructions.\n",
    "\n",
    "    Args:\n",
    "        x: Design matrix of shape (n_examples, dim).\n",
    "        w: Initial weight matrix of shape (n_examples, k).\n",
    "        phi: Initial mixture prior, of shape (k,).\n",
    "        mu: Initial cluster means, list of k arrays of shape (dim,).\n",
    "        sigma: Initial cluster covariances, list of k arrays of shape (dim, dim).\n",
    "\n",
    "    Returns:\n",
    "        Updated weight matrix of shape (n_examples, k) resulting from EM algorithm.\n",
    "        More specifically, w[i, j] should contain the probability of\n",
    "        example x^(i) belonging to the j-th Gaussian in the mixture.\n",
    "    \"\"\"\n",
    "    # No need to change any of these parameters\n",
    "    eps = 1e-3  # Convergence threshold\n",
    "    max_iter = 1000\n",
    "\n",
    "    # Stop when the absolute change in log-likelihood is < eps\n",
    "    # See below for explanation of the convergence criterion\n",
    "    it = 0\n",
    "    ll = prev_ll = None\n",
    "    while it < max_iter and (prev_ll is None or np.abs(ll - prev_ll) >= eps):\n",
    "        pass  # Just a placeholder for the starter code\n",
    "        # *** START CODE HERE\n",
    "        for i in range(K):\n",
    "        \tw[:, i] = np.exp(-0.5 * ((x-mu[i]).dot(np.linalg.inv(sigma[i])) * (x-mu[i])).sum(axis=1)) / (np.linalg.det(sigma[i])**0.5) * phi[i]\n",
    "        w /= w.sum(axis=1)[:, None]\n",
    "        # M-step\n",
    "        phi = np.mean(w, axis=0)\n",
    "        for i in range(K):\n",
    "        \tmu[i] = x.T.dot(w[:, i]) / sum(w[:, i])\n",
    "        \tsigma[i] = (w[:, i][:, None] * (x-mu[i])).T.dot(x-mu[i]) / sum(w[:, i])\n",
    "\n",
    "        it += 1\n",
    "        prev_ll = ll\n",
    "        p_xz = np.zeros(w.shape)\n",
    "        for i in range(K):\n",
    "        \tp_xz[:, i] = np.exp(-0.5 * ((x-mu[i]).dot(np.linalg.inv(sigma[i])) * (x-mu[i])).sum(axis=1)) / (np.linalg.det(sigma[i])**0.5) * phi[i]\n",
    "        ll = np.sum(np.log(p_xz))\n",
    "        # (1) E-step: Update your estimates in w\n",
    "        # (2) M-step: Update the model parameters phi, mu, and sigma\n",
    "        # (3) Compute the log-likelihood of the data to check for convergence.\n",
    "        # By log-likelihood, we mean `ll = sum_x[log(sum_z[p(x|z) * p(z)])]`.\n",
    "        # We define convergence by the first iteration where abs(ll - prev_ll) < eps.\n",
    "        # Hint: For debugging, recall part (a). We showed that ll should be monotonically increasing.\n",
    "        # *** END CODE HERE ***\n",
    "\n",
    "    return w\n",
    "\n",
    "\n",
    "def run_semi_supervised_em(x, x_tilde, z_tilde, w, phi, mu, sigma):\n",
    "    \"\"\"Problem 3(e): Semi-Supervised EM Algorithm.\n",
    "\n",
    "    See inline comments for instructions.\n",
    "\n",
    "    Args:\n",
    "        x: Design matrix of unlabeled examples of shape (n_examples_unobs, dim).\n",
    "        x_tilde: Design matrix of labeled examples of shape (n_examples_obs, dim).\n",
    "        z_tilde: Array of labels of shape (n_examples_obs, 1).\n",
    "        w: Initial weight matrix of shape (n_examples, k).\n",
    "        phi: Initial mixture prior, of shape (k,).\n",
    "        mu: Initial cluster means, list of k arrays of shape (dim,).\n",
    "        sigma: Initial cluster covariances, list of k arrays of shape (dim, dim).\n",
    "\n",
    "    Returns:\n",
    "        Updated weight matrix of shape (n_examples, k) resulting from semi-supervised EM algorithm.\n",
    "        More specifically, w[i, j] should contain the probability of\n",
    "        example x^(i) belonging to the j-th Gaussian in the mixture.\n",
    "    \"\"\"\n",
    "    # No need to change any of these parameters\n",
    "    alpha = 20.  # Weight for the labeled examples\n",
    "    eps = 1e-3   # Convergence threshold\n",
    "    max_iter = 1000\n",
    "\n",
    "    # Stop when the absolute change in log-likelihood is < eps\n",
    "    # See below for explanation of the convergence criterion\n",
    "    it = 0\n",
    "    ll = prev_ll = None\n",
    "    while it < max_iter and (prev_ll is None or np.abs(ll - prev_ll) >= eps):\n",
    "        pass  # Just a placeholder for the starter code\n",
    "        # *** START CODE HERE ***\n",
    "        # (1) E-step: Update your estimates in w\n",
    "        # (2) M-step: Update the model parameters phi, mu, and sigma\n",
    "        # (3) Compute the log-likelihood of the data to check for convergence.\n",
    "        # Hint: Make sure to include alpha in your calculation of ll.\n",
    "        # Hint: For debugging, recall part (a). We showed that ll should be monotonically increasing.\n",
    "        # *** END CODE HERE ***\n",
    "\n",
    "    return w\n",
    "\n",
    "\n",
    "# *** START CODE HERE ***\n",
    "# Helper functions\n",
    "# *** END CODE HERE ***\n",
    "\n",
    "\n",
    "def plot_gmm_preds(x, z, with_supervision, plot_id):\n",
    "    \"\"\"Plot GMM predictions on a 2D dataset `x` with labels `z`.\n",
    "\n",
    "    Write to the output directory, including `plot_id`\n",
    "    in the name, and appending 'ss' if the GMM had supervision.\n",
    "\n",
    "    NOTE: You do not need to edit this function.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.title('{} GMM Predictions'.format('Semi-supervised' if with_supervision else 'Unsupervised'))\n",
    "    plt.xlabel('x_1')\n",
    "    plt.ylabel('x_2')\n",
    "\n",
    "    for x_1, x_2, z_ in zip(x[:, 0], x[:, 1], z):\n",
    "        color = 'gray' if z_ < 0 else PLOT_COLORS[int(z_)]\n",
    "        alpha = 0.25 if z_ < 0 else 0.75\n",
    "        plt.scatter(x_1, x_2, marker='.', c=color, alpha=alpha)\n",
    "\n",
    "    file_name = 'pred{}_{}.pdf'.format('_ss' if with_supervision else '', plot_id)\n",
    "    save_path = os.path.join('.', file_name)\n",
    "    plt.savefig(save_path)\n",
    "\n",
    "\n",
    "def load_gmm_dataset(csv_path):\n",
    "    \"\"\"Load dataset for Gaussian Mixture Model.\n",
    "\n",
    "    Args:\n",
    "         csv_path: Path to CSV file containing dataset.\n",
    "\n",
    "    Returns:\n",
    "        x: NumPy array shape (n_examples, dim)\n",
    "        z: NumPy array shape (n_exampls, 1)\n",
    "\n",
    "    NOTE: You do not need to edit this function.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load headers\n",
    "    with open(csv_path, 'r') as csv_fh:\n",
    "        headers = csv_fh.readline().strip().split(',')\n",
    "\n",
    "    # Load features and labels\n",
    "    x_cols = [i for i in range(len(headers)) if headers[i].startswith('x')]\n",
    "    z_cols = [i for i in range(len(headers)) if headers[i] == 'z']\n",
    "\n",
    "    x = np.loadtxt(csv_path, delimiter=',', skiprows=1, usecols=x_cols, dtype=float)\n",
    "    z = np.loadtxt(csv_path, delimiter=',', skiprows=1, usecols=z_cols, dtype=float)\n",
    "\n",
    "    if z.ndim == 1:\n",
    "        z = np.expand_dims(z, axis=-1)\n",
    "\n",
    "    return x, z\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    np.random.seed(229)\n",
    "    # Run NUM_TRIALS trials to see how different initializations\n",
    "    # affect the final predictions with and without supervision\n",
    "    for t in range(NUM_TRIALS):\n",
    "        main(is_semi_supervised=False, trial_num=t)\n",
    "\n",
    "        # *** START CODE HERE ***\n",
    "        # Once you've implemented the semi-supervised version,\n",
    "        # uncomment the following line.\n",
    "        # You do not need to add any other lines in this code block.\n",
    "        # main(is_semi_supervised=True, trial_num=t)\n",
    "        # *** END CODE HERE ***\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
